{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOj8wAwbQegbcdVXfmscZ7R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OrvilleCunningham/Live-Session-Assignment-3-/blob/main/Lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##########PART 1\n",
        "def word_count_map(document):\n",
        "    \"\"\"\n",
        "    1.The word_count_map function should take a document as input and return a\n",
        "    list of tuples, each tuple containing a word and the number 1.\n",
        "    The number 1 represents the number of times the word appears\n",
        "    in the document. Here's an example implementation:\n",
        "    \"\"\"\n",
        "    return [(word, 1) for word in document.split()]\n",
        "\n",
        "def iterate(f,x,a):\n",
        "    if len(a)==0:\n",
        "        return x\n",
        "    else:\n",
        "        return iterate(f,f(x,a[0]),a[1:])\n",
        "def flatten(sequences):\n",
        "    return iterate(plus,[],sequences)\n",
        "def collect(pairs):\n",
        "    result =defaultdict(list)\n",
        "    for pair in sorted(pairs):\n",
        "        result[pair[0]].append(pair[1])\n",
        "    return list(result.items())\n",
        "def plus(x,y):\n",
        "    return x+y\n",
        "\n",
        "def word_count_reduce(p):\n",
        "    \"\"\"\n",
        "    2.The word_count_reduce function should take a word and a list of counts\n",
        "    as input and return a tuple with the word and the sum of\n",
        "    the counts. This function will be called for each word, and the list\n",
        "    of counts will be all the 1s returned by the word_count_map\n",
        "    function for that word. Here's an example implementation:\n",
        "    \"\"\"\n",
        "    word,counts=p\n",
        "    return (word, sum(counts))\n",
        "from collections import defaultdict\n",
        "def run_map_reduce(map_f, reduce_f, docs):\n",
        "    pairs = flatten(list(map(map_f,docs)))\n",
        "    groups =collect(pairs)\n",
        "    return [reduce_f(g) for g in groups]\n",
        "\"\"\"\n",
        "3.To test the full solution, you can call run_map_reduce with your word_count_map\n",
        "and word_count_reduce functions and the list of documents as inputs:\n",
        "\"\"\"\n",
        "docs = ['i am sam i am', 'sam is ham']\n",
        "result = run_map_reduce(word_count_map, word_count_reduce, docs)\n",
        "print(result)\n",
        "\"\"\"\n",
        "4.The work of word_count_reduce for a word w with n appearances would be proportional to n,\n",
        "since the function needs to sum up n counts. The span would be proportional to log(n),\n",
        "since the reduce function can be parallelized by\n",
        "dividing the list of counts into smaller sublists and reducing them in parallel.\n",
        "\n",
        "5.The problem with the solution that uses a loop and a dictionary to count\n",
        "words is that it cannot be easily parallelized. This is because the updates\n",
        "to the dictionary cannot be safely performed by multiple threads or processes\n",
        "at the same time, as it would lead to race conditions and incorrect results.\n",
        "Map-reduce is a way to solve this problem by splitting the data into chunks\n",
        "that can be processed in parallel, then combining the results.\n",
        "\"\"\"\n",
        "\n",
        "###PART 2\n",
        "def sentiment_map(doc,positive_terms = ['good', 'great', 'awesome','sockdolager'],\n",
        "                  negative_terms = ['bad', 'terrible',  'waste', 'carbuncle','corrupted']):\n",
        "    \n",
        "    result = []\n",
        "    for term in doc.split():\n",
        "        if term in positive_terms:\n",
        "            result.append(('positive', 1))\n",
        "        elif term in negative_terms:\n",
        "            result.append(('negative', 1))\n",
        "    return result\n",
        "\n",
        "def sentiment(docs):\n",
        "    return run_map_reduce(sentiment_map, word_count_reduce, docs)\n",
        "\n",
        "\n",
        "print(sentiment_map(\"it was a terrible waste of time\")==[('negative', 1), ('negative', 1)])\n",
        "docs =[ \"it was not great but no terrible\",\n",
        "        'thou art a boil a plague-sore or embossed carbuncle in my corrupted blood',\n",
        "        'it was a sockdolager of a good time']\n",
        "print(sentiment(docs))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wZAe5l6ddXa",
        "outputId": "d7468c7e-f184-4f1f-ad11-fc6a92e52377"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('am', 2), ('ham', 1), ('i', 2), ('is', 1), ('sam', 2)]\n",
            "True\n",
            "[('negative', 3), ('positive', 3)]\n"
          ]
        }
      ]
    }
  ]
}